{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhimanTarafdar/medical-insurance-cost-prediction-explore/blob/main/Medical_Insurance_Cost_Prediction_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlqCQ7zmxE3U"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "# Regression models\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
        "# Metrics\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "# Cross-validation and tuning\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haMIrEcxxuEA"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/medical_insurance.csv\")\n",
        "\n",
        "# Basic information\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Total rows: {len(df)}\")\n",
        "print(f\"Total columns: {len(df.columns)}\")\n",
        "print(\"\\n\")\n",
        "print(\"Column names:\")\n",
        "print(df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7TPpf8UzItz"
      },
      "outputs": [],
      "source": [
        "!pip install ydata-profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO_mJ1NFzV4J"
      },
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "profile = ProfileReport( df , title=\"Medical Insurance Cost Prediction\", explorative = True  )\n",
        "\n",
        "profile.to_file(\"ydata.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrH0eIKz0cvR"
      },
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "print(\"First 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Data types and missing values\n",
        "print(\"Data types and missing values:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Statistical summary\n",
        "print(\"Statistical Summary:\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Missing values count\n",
        "print(\"Missing values count:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Check for duplicate rows\n",
        "print(f\"Duplicate rows: {df.duplicated().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSTtZ_lk1Hjx"
      },
      "outputs": [],
      "source": [
        "df[df.duplicated()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64x_KDjR1LUp"
      },
      "outputs": [],
      "source": [
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQNm8bZq1OWF"
      },
      "outputs": [],
      "source": [
        "print(\"Duplicate rows after removal:\", df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAtBn0Sl1Snh"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1cshEAj1kSv"
      },
      "outputs": [],
      "source": [
        "# Check unique values in categorical columns\n",
        "\n",
        "print(\"Unique values in categorical columns:\")\n",
        "\n",
        "print(f\"Sex: {df['sex'].nunique()} unique sex\")\n",
        "print(f\"Region: {df['region'].nunique()} unique region\")\n",
        "print(f\"Smoker: {df['smoker'].nunique()} unique smoker\")\n",
        "\n",
        "\n",
        "# Check some categorical column values\n",
        "print(\"Sex Values:\")\n",
        "print(df['sex'].value_counts())\n",
        "\n",
        "print(\"\\nRegion Values:\")\n",
        "print(df['region'].value_counts())\n",
        "\n",
        "print(\"\\nSmoker Values:\")\n",
        "print(df['smoker'].value_counts())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37NA7RmA4iuI"
      },
      "outputs": [],
      "source": [
        "# Target variable distribution\n",
        "print(\"Target variable (charges) distribution:\")\n",
        "print(df['charges'].describe())\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Correlation with target (numeric features only)\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "corr_with_charges= df[numeric_cols].corr()['charges'].sort_values(ascending=False)\n",
        "\n",
        "print(\"Correlation with charges:\")\n",
        "print(corr_with_charges)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Visualize charges distribution\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(df['charges'], bins=50, color='skyblue', edgecolor='black')\n",
        "plt.xlabel('charges')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('charges Distribution')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.boxplot(df['charges'])\n",
        "plt.ylabel('charges')\n",
        "plt.title('charges Boxplot (Outlier Check)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Check for outliers\n",
        "Q1 = df['charges'].quantile(0.25)\n",
        "Q3 = df['charges'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "outliers = df[(df['charges'] < lower_bound) | (df['charges'] > upper_bound)]\n",
        "print(f\"Number of outliers in charges: {len(outliers)}\")\n",
        "print(f\"Percentage of outliers: {(len(outliers)/len(df)*100):.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhMQzR4g5ugc"
      },
      "outputs": [],
      "source": [
        "# Select only numeric columns for correlation\n",
        "numeric_data = df.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "# Create correlation matrix\n",
        "corr_matrix = numeric_data.corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
        "            center=0, square=True, linewidths=1)\n",
        "plt.title('Correlation Heatmap - Numeric Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Show correlation with charges in detail\n",
        "print(\"Correlation with charges (sorted):\")\n",
        "print(corr_matrix['charges'].sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4rnUN5L7DEC"
      },
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X = df.drop('charges', axis=1)\n",
        "y = df['charges']\n",
        "\n",
        "print(\"Features (X):\")\n",
        "print(X.head())\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Target (y):\")\n",
        "print(y.head())\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Identify numeric and categorical features\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
        "print(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Check categorical feature\n",
        "print(\"Categorical features - unique values:\")\n",
        "for col in categorical_features:\n",
        "    print(f\"{col}: {X[col].nunique()} unique values\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV-iTY7W8Nuu"
      },
      "outputs": [],
      "source": [
        "# Numeric transformer - impute + scale\n",
        "num_transformer = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Categorical transformer - impute + encode\n",
        "cat_transformer = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Combine transformers\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', num_transformer, numeric_features),\n",
        "        ('cat', cat_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Preprocessing pipeline created!\")\n",
        "print(\"\\nNumeric features will be:\")\n",
        "print(\"  - Imputed with median\")\n",
        "print(\"  - Scaled with StandardScaler\")\n",
        "\n",
        "print(\"\\nCategorical features will be:\")\n",
        "print(\"  - Imputed with most_frequent\")\n",
        "print(\"  - Encoded with OneHotEncoder\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
        "print(f\"Train-test ratio: 80-20\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtwJp3rl9COl"
      },
      "outputs": [],
      "source": [
        "# Define base models\n",
        "reg_lr = LinearRegression()\n",
        "reg_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "reg_gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "reg_knn = KNeighborsRegressor(n_neighbors=5,weights='distance')\n",
        "\n",
        "# Voting Regressor - average of all models\n",
        "voting_reg = VotingRegressor(\n",
        "    estimators=[\n",
        "        ('lr', reg_lr),\n",
        "        ('rf', reg_rf),\n",
        "        ('gb', reg_gb),\n",
        "        ('knn', reg_knn),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Stacking Regressor - meta learner\n",
        "stacking_reg = StackingRegressor(\n",
        "    estimators=[\n",
        "        ('rf', reg_rf),\n",
        "        ('gb', reg_gb),\n",
        "        ('knn', reg_knn),\n",
        "\n",
        "    ],\n",
        "    final_estimator=Ridge()\n",
        ")\n",
        "\n",
        "# Dictionary of all models\n",
        "models_to_train = {\n",
        "    'Linear Regression': reg_lr,\n",
        "    'Random Forest': reg_rf,\n",
        "    'Gradient Boosting': reg_gb,\n",
        "    'KNN Regression': reg_knn,\n",
        "    'Voting Ensemble': voting_reg,\n",
        "    'Stacking Ensemble': stacking_reg\n",
        "}\n",
        "\n",
        "print(\"6 models defined!\")\n",
        "print(\"\\nModels to train:\")\n",
        "for i, name in enumerate(models_to_train.keys(), 1):\n",
        "    print(f\"  {i}. {name}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Train and evaluate all models\n",
        "results = []\n",
        "\n",
        "for name, model in models_to_train.items():\n",
        "    print(f\"Training {name}...\")\n",
        "\n",
        "    # Create pipeline\n",
        "    pipe = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    # Train\n",
        "    pipe.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = pipe.predict(X_test)\n",
        "\n",
        "    # Evaluate\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'R2 Score': r2,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae\n",
        "    })\n",
        "\n",
        "    print(f\"{name} trained - R2: {r2:.4f}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Create results dataframe\n",
        "results_df = pd.DataFrame(results).sort_values('R2 Score', ascending=False)\n",
        "\n",
        "print(\"MODEL COMPARISON RESULTS:\")\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ny_ZoAP_pqY"
      },
      "outputs": [],
      "source": [
        "# Select best model\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "best_model_obj = models_to_train[best_model_name]\n",
        "\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(f\"R2 Score: {results_df.iloc[0]['R2 Score']:.4f}\")\n",
        "print(f\"RMSE: {results_df.iloc[0]['RMSE']:.2f}\")\n",
        "print(f\"MAE: {results_df.iloc[0]['MAE']:.2f}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Train best model on full pipeline\n",
        "final_pipe = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', best_model_obj)\n",
        "])\n",
        "\n",
        "final_pipe.fit(X_train, y_train)\n",
        "y_final_pred = final_pipe.predict(X_test)\n",
        "\n",
        "# Plot Actual vs Predicted\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "sns.scatterplot(x=y_test, y=y_final_pred, alpha=0.6, color='teal')\n",
        "plt.plot([y_test.min(), y_test.max()],\n",
        "         [y_test.min(), y_test.max()],\n",
        "         color='red', linestyle='--', linewidth=2, label='Perfect Prediction')\n",
        "\n",
        "plt.xlabel('Actual charges', fontsize=12)\n",
        "plt.ylabel('Predicted charges', fontsize=12)\n",
        "plt.title(f'Actual vs Predicted charges - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Show some sample predictions\n",
        "sample_results = pd.DataFrame({\n",
        "    'Actual charges': y_test.head(10).values,\n",
        "    'Predicted charges': y_final_pred[:10].astype(int),\n",
        "    'Difference': (y_test.head(10).values - y_final_pred[:10]).astype(int)\n",
        "})\n",
        "\n",
        "print(\"Sample Predictions (First 10):\")\n",
        "print(sample_results.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB4r6eQ-ApCV"
      },
      "outputs": [],
      "source": [
        "# Cross-validation with Gradient Boosting\n",
        "gb_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', GradientBoostingRegressor(n_estimators=100,random_state=42))\n",
        "])\n",
        "\n",
        "print(\"Performing 5-Fold Cross-Validation on Gradient Boosting...\")\n",
        "\n",
        "# 5-fold cross-validation\n",
        "cv_scores = cross_val_score(\n",
        "    gb_pipeline,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error'\n",
        ")\n",
        "\n",
        "cv_rmse = np.sqrt(-cv_scores)\n",
        "\n",
        "print(\"Cross-Validation Results:\")\n",
        "print(f\"RMSE scores for each fold: {cv_rmse}\")\n",
        "print(f\"\\nMean RMSE: {cv_rmse.mean():.2f}\")\n",
        "print(f\"Std RMSE: {cv_rmse.std():.2f}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Also check R2 with cross-validation\n",
        "cv_r2_scores = cross_val_score(\n",
        "    gb_pipeline,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    cv=5,\n",
        "    scoring='r2'\n",
        ")\n",
        "\n",
        "print(\"R2 scores for each fold:\", cv_r2_scores)\n",
        "print(f\"Mean R2: {cv_r2_scores.mean():.4f}\")\n",
        "print(f\"Std R2: {cv_r2_scores.std():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "gb_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = gb_pipeline.predict(X_train)\n",
        "y_test_pred  = gb_pipeline.predict(X_test)\n",
        "\n",
        "\n",
        "# Metrics\n",
        "def regression_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"R2\": r2_score(y_true, y_pred),\n",
        "        \"RMSE\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "        \"MAE\": mean_absolute_error(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "train_metrics = regression_metrics(y_train, y_train_pred)\n",
        "test_metrics  = regression_metrics(y_test, y_test_pred)\n",
        "\n",
        "\n",
        "# Print Results\n",
        "print(\"TRAIN PERFORMANCE\")\n",
        "print(f\"R2   : {train_metrics['R2']:.4f}\")\n",
        "print(f\"RMSE : {train_metrics['RMSE']:.2f}\")\n",
        "print(f\"MAE  : {train_metrics['MAE']:.2f}\")\n",
        "\n",
        "print(\"\\nTEST PERFORMANCE\")\n",
        "print(f\"R2   : {test_metrics['R2']:.4f}\")\n",
        "print(f\"RMSE : {test_metrics['RMSE']:.2f}\")\n",
        "print(f\"MAE  : {test_metrics['MAE']:.2f}\")\n",
        "\n",
        "\n",
        "# Overfitting Check\n",
        "print(\"\\nOVERFITTING CHECK\")\n",
        "print(f\"R2 Difference   : {train_metrics['R2'] - test_metrics['R2']:.4f}\")\n",
        "print(f\"RMSE Difference: {test_metrics['RMSE'] - train_metrics['RMSE']:.2f}\")\n",
        "print(f\"MAE Difference : {test_metrics['MAE'] - train_metrics['MAE']:.2f}\")\n"
      ],
      "metadata": {
        "id": "H7LfbLo6kfHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucC853pjBV2w"
      },
      "outputs": [],
      "source": [
        "# Get before tuning metrics FIRST\n",
        "print(\"Training baseline model...\")\n",
        "gb_pipeline.fit(X_train, y_train)\n",
        "y_pred_before = gb_pipeline.predict(X_test)\n",
        "before_r2 = r2_score(y_test, y_pred_before)\n",
        "before_rmse = np.sqrt(mean_squared_error(y_test, y_pred_before))\n",
        "before_mae = mean_absolute_error(y_test, y_pred_before)\n",
        "\n",
        "print(f\"Baseline Performance - R2: {before_r2:.4f}, RMSE: {before_rmse:.2f}, MAE: {before_mae:.2f}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Define parameter grid for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'model__n_estimators': randint(100, 300),\n",
        "    'model__max_depth': [3, 5, 7, 10],\n",
        "    'model__min_samples_split': randint(2, 10),\n",
        "    'model__learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "print(\"Starting Hyperparameter Tuning...\")\n",
        "\n",
        "# RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=gb_pipeline,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    cv=5,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Best Parameters Found:\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "print(f\"\\nBest Cross-Validation RMSE: {-random_search.best_score_:.2f}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Evaluate tuned model on test set\n",
        "y_tuned_pred = random_search.predict(X_test)\n",
        "\n",
        "tuned_r2 = r2_score(y_test, y_tuned_pred)\n",
        "tuned_rmse = np.sqrt(mean_squared_error(y_test, y_tuned_pred))\n",
        "tuned_mae = mean_absolute_error(y_test, y_tuned_pred)\n",
        "\n",
        "print(\"Tuned Model Performance on Test Set:\")\n",
        "print(f\"R2 Score: {tuned_r2:.4f}\")\n",
        "print(f\"RMSE: {tuned_rmse:.2f}\")\n",
        "print(f\"MAE: {tuned_mae:.2f}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Compare before and after tuning\n",
        "print(\"COMPARISON: Before vs After Tuning\")\n",
        "print(f\"{'Metric':<15} {'Before':<15} {'After':<15} {'Improvement'}\")\n",
        "print(\"\\n\")\n",
        "print(f\"{'R2 Score':<15} {before_r2:<15.4f} {tuned_r2:<15.4f} {'+' if tuned_r2 > before_r2 else ''}{(tuned_r2 - before_r2):.4f}\")\n",
        "print(f\"{'RMSE':<15} {before_rmse:<14.2f} {tuned_rmse:<14.2f} {before_rmse - tuned_rmse:.2f}\")\n",
        "print(f\"{'MAE':<15} {before_mae:<14.2f} {tuned_mae:<14.2f} {before_mae - tuned_mae:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#best tuned model\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Train predictions\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "\n",
        "# Test predictions\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "\n",
        "# Train metrics\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "\n",
        "# Test metrics\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "print(\"TRAIN PERFORMANCE\")\n",
        "print(f\"R2   : {train_r2:.4f}\")\n",
        "print(f\"RMSE : {train_rmse:.2f}\")\n",
        "print(f\"MAE  : {train_mae:.2f}\")\n",
        "\n",
        "print(\"\\nTEST PERFORMANCE\")\n",
        "print(f\"R2   : {test_r2:.4f}\")\n",
        "print(f\"RMSE : {test_rmse:.2f}\")\n",
        "print(f\"MAE  : {test_mae:.2f}\")\n",
        "\n",
        "print(\"\\nOVERFITTING CHECK\")\n",
        "print(f\"R2 Difference   : {train_r2 - test_r2:.4f}\")\n",
        "print(f\"RMSE Difference: {test_rmse - train_rmse:.2f}\")\n",
        "print(f\"MAE Difference : {test_mae - train_mae:.2f}\")\n"
      ],
      "metadata": {
        "id": "zEkly0OnIY4x"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTB8RMAYgyWvBAD5cJvV2Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}